{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"assignment1.intent-slot-identification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"18OWpGevfEBWlZ1G_3IhfmQpKD8IzipQK","authorship_tag":"ABX9TyO5tLwzHNVqqqaBCdeF2bOB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"adp5BudsY7Zo"},"source":["# Implementation of NLU component of a chatbot\n","\n","In this assignment you will build an application that performs __intent classification__ and __slot filling__, which are the core tasks that take care the **NLU module** in any chatbot. \n","\n","*   __intent classification__ consist in mapping user utterance (typically a \n","customer) into a predefined set of _intents_ (goals of the user) so the computer can know what she is asking.\n","\n","* __slot filling__ consist in extracting filler values from the uterance of the user. \n","\n","The example below show a user command asking a flight ticket to fly from Boston to Denver, departing at 8:38 and arriving at 11:10. The goal is to extrac all this information automatically. \n","\n","![](https://drive.google.com/uc?export=view&id=1cWS1ybRPMGladx3qv8G4th5fyZO3cA-h)\n","\n","\n","We will use the [ATIS (Airline Travel Information Systems) dataset](https://www.aclweb.org/anthology/H90-1021.pdf), which is  a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories, and 81 slots in total. \n","\n","The main goal of the assigment is to build a system that perform intent detection and slot filing. We define a list steps you can folllow to complete the assignemnt. The steps are the following:\n","- Preprocess the data.\n","- Implement the models for intent classification and slot filling.\n","- Evaluate proposed model.\n","- Run some analysis of the model output.\n","\n","Note you are not requiered to strictly follow the steps suggested in the notebook as long as you build a model, evaluate it and provide some analysis.\n"]},{"cell_type":"markdown","metadata":{"id":"h3cOrUAxZ3zk"},"source":["## Some utils\n","We provide some utilities that can be useful for the assignments. You are not obligated to use them, but depending on you model (e.g. CRF, LSTM, or BERT), it might be a good idea to use them. Also, it might faster to  \n","\n","We provide two \n"]},{"cell_type":"code","metadata":{"id":"Lulq5OztYDHA"},"source":["import pandas as pd\n","\n","# helper function to read dev and test data format (use get_data2 for training)\n","def get_data(filename):\n","    df = pd.read_csv(filename, delim_whitespace=True, names=['word', 'label'])\n","    beg_indices = list(df[df['word'] == 'BOS'].index)+[df.shape[0]]\n","    sents, labels, intents = [], [], []\n","    for i in range(len(beg_indices[:-1])):\n","        sents.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['word'].values)\n","        labels.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['label'].values)\n","        intents.append(df.loc[beg_indices[i+1]-1]['label'])    \n","    return sents, labels, intents\n","\n","# helper function to read training data format\n","def get_data2(filename):\n","    with open(filename) as f:\n","        contents = f.read()\n","    sents, labels, intents = [],[],[]\n","    for line in contents.strip().split('\\n'):\n","        words, labs = [i.split(' ') for i in line.split('\\t')]\n","        sents.append(words[1:-1])\n","        labels.append(labs[1:-1])\n","        intents.append(labs[-1])\n","    return sents, labels, intents"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LiuzPmPZnBC"},"source":["## Load the data and understand it\n","\n","First thing you need to do is to understand the different aspects of the your data, such as knowing how the information is formated, the number of examples we have to train, validate and test our models, what is the distribution of labels, and so on. \n","\n","### TO-DO\n","Using your own code or the one provided above load the data and run some small analysis. For example, it is a good idea to print some statistics: \n","\n","- Number of examples for each partition\n","- Class distribution for intent classification\n","- Class distribution for slot filling task. \n","- Do we have same set of slot labels for each intent type? \n","\n","Additionally, it is also good idea to create some plots of the statistics. "]},{"cell_type":"code","metadata":{"id":"PwuqHDG_d8Vq"},"source":["data_dir=\"drive/My Drive/Colab Notebooks/nlp-app-II/data/atis\"\n","train_file= data_dir + '/atis.train.w-intent.iob'\n","dev_file= data_dir + '/atis-2.dev.w-intent.iob'\n","test_file= data_dir + '/atis.test.w-intent.iob'\n","\n","train_texts, train_slots, train_intents = get_data2(train_file)\n","dev_texts, dev_slots, dev_intents = get_data(dev_file)\n","test_text, test_slots, test_intents = get_data(test_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCTc0U3bhVfK"},"source":["print(train_texts[0])\n","print(train_slots[0])\n","print(train_intents[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKIOEMzZZ4EU"},"source":["## Preprocess\n","\n","As we learn in class all the NLP task require some sort of preprocessing steps. The steps vary depending on the task as well as the type model we plan to use. That is, if our choice is to train a feature based ML model, then we probably would need more than tokenization (recall that based on the preprocessing we will build our set of features). As the goal of the assignment is not to exclusively focus on preprocessing, you can start tokenizating, lemmatizing and getting the PoS of each token. For this, you can any of the existing tool for NLP such as spaCy and NLTK.\n","\n","On the contrary, if we plan to use DL for modeling our tasks, than preprocessing steps can be reduced to: \n","- Tokenization\n","- Word indexing (and creation of the vocabulary)\n","- Converting text to sequence of word indexes (vectorization)\n","- Sequence padding if needed.\n","\n","### TO-DO\n","- Preprare your data to feed your model."]},{"cell_type":"code","metadata":{"id":"kak-0ve4Z-zy"},"source":["def preprocess_data(data_list):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m4c1ky1JZ-Kw"},"source":["## Build your models\n","\n","Note that in to complete the assignment you have to be able to approach the two tasks, which typically they are approached in separated manner. _Intent classification_ can be cast as __text classification__, while _slot filling_ is usually approached as a __squence labeling__ task. \n","The simplest solution is to create two models, where we learn each task separately. Optionaly, you can define a joint model in which both tasks are learned at the same time. \n","\n","The following code shows the set of methods that models should contain.\n","\n","- `__init__`: You can use the initialization method (or constructor) to set the hyperparameters of the model.\n","- `fit`: Method that takes care of the training. Tipically it takes training and validation data as input.\n","- `predict`: Given the test data it returns the predictions of the model.\n","\n","In the case you are using a Python library like `Scikit-learn` you really don't need to use the code snippets provided below. Then you can directly load your ML object. \n","\n","### TO-DO\n","\n","- Generate the code to build the models that are able to approach the inten classification  and slot filling tasks.\n","- You can reused the code used in the other labs. \n","\n"]},{"cell_type":"code","metadata":{"id":"iGIVJNlVaa_-"},"source":["# Use this snippet code as a guide, and feel free to modify it as you want.\n","\n","class text_classification_model(object):\n","    def __init__():\n","        pass\n","\n","    def fit(train_data, val_data):\n","        pass\n","\n","    def predict(test_data):\n","        pass\n","\n","\n","class sequence_labeling_model(object):\n","    def __init__():\n","        pass\n","\n","    def fit(train_data, val_data):\n","        pass\n","\n","    def predict(test_data):\n","        pass "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yhy0NLfeabs3"},"source":["## Train, validate and evaluate\n","\n","__Training__\n","\n","Once you defined your model, and loaded and preprocessed the data, you just need to train it.  Depending on your model choice, you might need to extract the features (and vectorize the examples) before you fit your model. \n","\n","Be aware that in this phase we can explore and tune the hyperparameter values of our model. Note that we are not allowed to use the test partition for that. All the development of our approach is done on training and development paritions. \n","\n","__Evaluation__\n","\n","Once you fit the model and you are happy with your results obtained in the validation part, you can move to evaluate the model on the test part. \n"," Note that you have to choose a set of useful evaluation __metrics__. Typically, we use precision, recall, and f1-score, and you can use  For that you can use the functions in `sklearn.metrics` modules.  If you are using `tensorflow` or `pytorch` you can their buil-in metrics for the evaluation.\n","\n","### TO-DO\n","- Fit and tune your model using the training and development partition.\n","- Make clear in the code how you tune your model. \n","- Evaluate it on the test set.\n","\n"]},{"cell_type":"code","metadata":{"id":"KZ9mP-IAakhK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvM6t1RxcUk7"},"source":["## Analysis of the output\n","It is important to run some analysis of the output of our model in order to gain some insights about good and bad things of the model. A good idea is to extract some correct examples and incorrectly predicted examples. For the slot filing task you can use [the spaCy visualizer for entities](https://spacy.io/usage/visualizers). A example of how to use it you can find in the notebook of __lab 2 (NER with spaCy)__.\n","\n","In addition, the calculation of a confusion matrix it is very helpful to depict the type of prediction error. It helps to see if there a bias towards a specific set of labels, or there exist some kind confusability between classes (e.g. because class definitions are similar) and so on.  You can find in `scikit-learn` a module to calculate confusion matrices (`sklearn.metrics.confusion_matrix`)\n","\n","### TO-DO \n","- Show some correct and incorrect examples for intent classification and slot filling. \n","- Calculate on the development set and show the confusion matrix of your model. \n"]},{"cell_type":"code","metadata":{"id":"NHcEj6lk9AEH"},"source":[""],"execution_count":null,"outputs":[]}]}