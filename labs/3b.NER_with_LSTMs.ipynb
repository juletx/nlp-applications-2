{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3b.NER_with_LSTMs.ipynb","provenance":[{"file_id":"1c88Hin0bnoBq0FczLgFI8qGtShxBcftO","timestamp":1647962747364}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMEBfgj5eDhv+cnDEo0VDi1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5u1mSr46G2-x"},"source":["# Training and Evaluating a LSTM model on CONLL2003\n","In this lab we will build our own model for NER. In this case, we will use an implementation of a LSTM in TensorFlow. As in the previous lab we will use the CONLL 2003 dataset to train and evaluate our model."]},{"cell_type":"markdown","metadata":{"id":"4xD9mRSOHNTO"},"source":["## Set-up\n","In this section we will set up the notebook by mounting the drive, doing all the required imports. We are going to use WandB (https://wandb.ai) for monitoring our model. You will need an account in WandB. If you prefer not having an account in WandB you can skip the parts related to WandB. Also you'll need to modify the code. \n","\n","Follow the instruction to create an account in the following url: https://app.wandb.ai/login?signup=true\n"]},{"cell_type":"code","metadata":{"id":"cqs9yaiNJRTk"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WandB â€“ Install the W&B library\n","!pip install wandb -q\n","import wandb\n","from wandb.keras import WandbCallback"],"metadata":{"id":"Mtk9NGw-j9Os"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4Jr5wdDY1v1"},"source":["## Loading the data\n","In this section we provide a function (`load_data_conll`) to load the data in CONLL format, in which we have each token per line along with multiple levels of annotations. The file contains a format of 4 whitespace separated colums(words, PoS, Chunk and NE tags). The function outputs a list where each item is composed of 2 lists: 1) a sentence as list of tokens, and NER tags a list of each token. For example:\n","\n","`[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']]`"]},{"cell_type":"code","metadata":{"id":"GWhyDST8Isr9"},"source":["\"\"\"\n","Load the training/testing data. \n","input: conll format data, with 4 whitespace separated colums - words, PoS, Chunk and NE tags.\n","output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n","\"\"\"\n","#functions for preparing the data in the *.txt files\n","def load_data_conll(file_path):\n","    myoutput,words,tags = [],[],[]\n","    fh = open(file_path)\n","    for line in fh:\n","        line = line.strip()\n","        if line.startswith(\"-DOCSTART\"):\n","            #skip -DOCSTART- and the next line\n","            fh.readline()\n","        elif line == \"\":\n","            #Sentence ended.\n","            myoutput.append([words,tags])\n","            words,tags = [],[]\n","        else:   \n","            parts = line.split()\n","            #word, pos_tag, chunk_tag, ner_tag = line.split()\n","            if len(parts) == 4:\n","                words.append(parts[0])\n","                tags.append(parts[-1])\n","    fh.close()\n","    return myoutput"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["work_dir = \"drive/MyDrive/Colab Notebooks/nlp-app-II/data\"\n","conll_dir = work_dir + \"/conll2003/en\"\n","train_path = conll_dir + \"/train.txt\"\n","dev_path = conll_dir + \"/valid.txt\"\n","test_path = conll_dir + \"/test.txt\"\n","\n","conll_train = load_data_conll(train_path)\n","conll_dev = load_data_conll(dev_path)"],"metadata":{"id":"uM6brSkBPmej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data preprocessing\n","In this section we provide the code to prepare the data-format useful for the LSTM architecture. \n","\n","Basically, data is tokenized, indexed and padded. For that we use tensorflow.keras tools."],"metadata":{"id":"YguSB1u8znna"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"rW4TdVpWSfUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Tokenized text: {}\".format(conll_train[0][0]))\n","print(\"Tokens labels: {}\".format(conll_train[0][1]))"],"metadata":{"id":"OvSs8hmtRZBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sentences = [\" \".join(sent[0]) for sent in conll_train]\n","dev_sentences = [\" \".join(sent[0]) for sent in conll_dev]\n","\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',lower=False, oov_token='<OOV>')\n","tokenizer.fit_on_texts(train_sentences)\n","\n","## text to word indices\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","dev_sequences = tokenizer.texts_to_sequences(dev_sentences)\n","\n","## padding\n","train_data = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=40)\n","dev_data = tf.keras.preprocessing.sequence.pad_sequences(dev_sequences, maxlen=40)"],"metadata":{"id":"FbOp4d0LSIFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process labels\n","labels = set([label for sent in conll_train for label in sent[1]])\n","label2index = {name: i for i, name in enumerate(sorted(labels))}\n","index2label = {label2index[name]:name for name in label2index}\n","train_labels = []\n","for sent in conll_train:\n","    train_labels.append([label2index[label] for label in sent[1]])\n","train_labels = tf.keras.preprocessing.sequence.pad_sequences(train_labels, maxlen=40)\n","\n","dev_labels = []\n","for sent in conll_dev:\n","    dev_labels.append([label2index[label] for label in sent[1]])\n","dev_labels = tf.keras.preprocessing.sequence.pad_sequences(dev_labels, maxlen=40)\n","\n","# one-hot encoding\n","train_labels = tf.keras.utils.to_categorical(train_labels)\n","dev_labels = tf.keras.utils.to_categorical(dev_labels)"],"metadata":{"id":"xwosMxmhojfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_labels.shape)\n","print(train_data.shape)\n","\n","print(dev_labels.shape)\n","print(dev_data.shape)"],"metadata":{"id":"W4NYysGI21QC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Model definition and training"],"metadata":{"id":"7yqRngNwk2b3"}},{"cell_type":"markdown","source":["First, we are going to initialize wandb session to monitor the evolution of our model, and sepecify the hyperparameters of our model in a config object. "],"metadata":{"id":"eVrrlrzECysA"}},{"cell_type":"code","source":["# Initilize a new wandb run\n","wandb.init(entity=\"oierldl\", project=\"ner-conll2003\")\n","\n","# Config is a variable that holds and saves hyperparameters and inputs\n","# Default values for hyper-parameters\n","config = wandb.config \n","config.learning_rate = 0.01\n","config.epochs = 5\n","config.num_classes = len(labels)\n","config.batch_size = 128\n","config.optimizer = 'adam'\n","config.seed = 42\n","config.vocab_size = len(tokenizer.index_word)+1\n","config.emb_size = 300\n","config.lstm_size = 128\n","config.dropout = 0.2"],"metadata":{"id":"5eLy9I9kkggw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model definition \n","The following lines of code defined LSTM based sequence labeler. We stack the following tf layers: 1) input layers that set the shape of the input and connects with the 2) embedding layer, which generates the input for the 3) bidirectional LSTM layer. Output of the LSTM is passed through a 4) dropout layer. Note that we set `return_sequences=true` so we can get the representation of each token and apply automatically 5) the dense layer for classification."],"metadata":{"id":"Mn_iE05QCtXn"}},{"cell_type":"code","source":["# model\n","model = tf.keras.models.Sequential([\n","            tf.keras.layers.Input(shape=(None,), dtype='int32', name='word_ids'),\n","            tf.keras.layers.Embedding(config.vocab_size, config.emb_size,\n","                                      mask_zero=True, trainable=True),\n","            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=config.lstm_size,\n","                                                               return_sequences=True)),\n","            tf.keras.layers.Dropout(config.dropout),\n","            tf.keras.layers.Dense(config.num_classes, activation='softmax')\n","           ])"],"metadata":{"id":"FKYmBWjllU9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compile model with ADAM optimizer\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate, ),\n","              loss= \"categorical_crossentropy\",\n","              metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n"],"metadata":{"id":"XsAvWvxQpVAJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"s5U6MKWpn8ej"}},{"cell_type":"code","source":["%%wandb\n","\n","model.fit(train_data, train_labels, \n","          batch_size=config.batch_size, epochs=config.epochs,\n","          validation_data=(dev_data, dev_labels) , callbacks=[WandbCallback()])\n"],"metadata":{"id":"RTsVMMT7HBQx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Make predictions"],"metadata":{"id":"5ptqfuMNp8Tf"}},{"cell_type":"code","source":["import numpy as np\n","\n","def make_predictions(x_test, y_test):\n","    preds = model.predict(x_test)\n","    preds = np.argmax(preds, axis=-1)\n","    labs = np.argmax(y_test, axis=-1)\n","    preds = [y[x != 0] for x, y in zip(x_test, preds)]\n","    labs = [y[x != 0] for x, y in zip(x_test, labs)]\n","    return preds, labs"],"metadata":{"id":"307cCdsrEnr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2xvcUf55-u0"},"source":["def dump_to_file(words, labels, preds):\n","    f = open('output.tsv', 'w', encoding='utf-8')\n","    for i in range(len(preds)):\n","        for w, l,p in zip(words[i],labels[i], preds[i]):\n","            f.write(w + \" \" + l + \" \" + p + \"\\n\")\n","        f.write('\\n')\n","    f.close()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions, ground_truth = make_predictions(dev_data, dev_labels)"],"metadata":{"id":"3QUTfkbEMWyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trues = [[index2label[i] for i in sentence] for sentence in ground_truth]\n","preds  = [[index2label[i] for i in sentence] for sentence in predictions]\n","words = [ sentence.split(\" \") for sentence in dev_sentences]"],"metadata":{"id":"XMNRn-MstMfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dump_to_file(words, trues, preds)"],"metadata":{"id":"tC939cQ112h3"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEQV5QytArsq"},"source":["!cp \"drive/MyDrive/00-Irakaskuntza/HAP-LAP-masterra/NLP-Applications-2/Part1: Information-extraction/notebooks/conlleval.txt\" ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdNSFx3J8Lb5"},"source":["!perl conlleval.txt < output.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dL-6mC6tfqbR"},"source":["from sklearn.metrics import make_scorer,confusion_matrix\n","\n","def print_cm(cm, labels):\n","    print(\"\\n\")\n","    \"\"\"pretty print for confusion matrixes\"\"\"\n","    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n","    empty_cell = \" \" * columnwidth\n","    # Print header\n","    print(\"    \" + empty_cell, end=\" \")\n","    for label in labels:\n","        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n","    print()\n","    # Print rows\n","    for i, label1 in enumerate(labels):\n","        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n","        sum = 0\n","        for j in range(len(labels)):\n","            cell = \"%{0}.0f\".format(columnwidth) % cm[i, j]\n","            sum =  sum + int(cell)\n","            print(cell, end=\" \")\n","        print(sum) #Prints the total number of instances per cat at the end.\n","\n","def get_confusion_matrix(y_true,y_pred,labels):\n","    trues,preds = [], []\n","    for yseq_true, yseq_pred in zip(y_true, y_pred):\n","        trues.extend(yseq_true)\n","        preds.extend(yseq_pred)\n","    print_cm(confusion_matrix(trues,preds,labels=labels),labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEtBtfYrfr6o"},"source":["def get_confusion_matrix(y_true,y_pred,labels):\n","    trues,preds = [], []\n","    for yseq_true, yseq_pred in zip(y_true, y_pred):\n","        trues.extend(yseq_true)\n","        preds.extend(yseq_pred)\n","    print_cm(confusion_matrix(trues,preds,labels=labels),labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_confusion_matrix(trues, preds, [label for label in label2index])"],"metadata":{"id":"4Ba3YiQWjqV5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gTSy95_w5Ua"},"source":["## Exercise 1\n","\n","Look at the confusion matrix. Note that, rows are gold labels and colums are the predicted labels. Where is located the confusion? "]},{"cell_type":"markdown","source":["## Exercise 2\n","\n","Try different hyperparameter settings to see if you are able to improve the results of the NER model.  "],"metadata":{"id":"0euNdKJS228a"}}]}