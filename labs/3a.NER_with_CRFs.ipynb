{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.NER_with_CRFs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZfT3tJkVkgP35hiohzL1D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5u1mSr46G2-x"},"source":["# Training and Evaluating a CRF model on CONLL2003\n","In this lab we will build our own model for NER. In this case, we will use an implementation of a CRF in Python, in which we have to build our set of learning features. As in the previous lab we will use the CONLL 2003 dataset to train and evaluate our model."]},{"cell_type":"markdown","metadata":{"id":"4xD9mRSOHNTO"},"source":["## Set-up\n","In this section we will set up the notebook by mounting the drive, doing all the required imports, and downloading the pos-tagger model in NLTK. \n","\n","We are going to install and use `sklearn-crfsuite` a [Python wrapper](https://github.com/TeamHG-Memex/sklearn-crfsuite) for the original [crfsuite](https://www.chokkan.org/software/crfsuite/)."]},{"cell_type":"code","metadata":{"id":"cqs9yaiNJRTk"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## need to downgrade sklearn to make compatible with crf-suit\n","pip install scikit-learn==0.24"],"metadata":{"id":"yM7DXk2ksNWX"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c24qghYzKr1M"},"source":["!pip install sklearn-crfsuite"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jHRwgIVEPqc"},"source":["import string\n","import numpy as np\n","\n","from sklearn_crfsuite import CRF, metrics\n","from sklearn.metrics import make_scorer,confusion_matrix\n","from sklearn.metrics import f1_score,classification_report\n","from sklearn.pipeline import Pipeline\n","\n","from pprint import pprint\n","\n","from nltk import download\n","from nltk.tag import pos_tag\n","download('averaged_perceptron_tagger') # download "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4Jr5wdDY1v1"},"source":["## Loading the data\n","In this section we provide a function (`load_data_conll`) to load the data in CONLL format, in which we have each token per line along with multiple levels of annotations. The file contains a format of 4 whitespace separated colums(words, PoS, Chunk and NE tags). The function outputs a list where each item is composed of 2 lists: 1) a sentence as list of tokens, and NER tags a list of each token. For example:\n","\n","`[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']]`"]},{"cell_type":"code","metadata":{"id":"GWhyDST8Isr9"},"source":["\"\"\"\n","Load the training/testing data. \n","input: conll format data, with 4 whitespace separated colums - words, PoS, Chunk and NE tags.\n","output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n","\"\"\"\n","#functions for preparing the data in the *.txt files\n","def load_data_conll(file_path):\n","    myoutput,words,tags = [],[],[]\n","    fh = open(file_path)\n","    for line in fh:\n","        line = line.strip()\n","        if line.startswith(\"-DOCSTART\"):\n","            #skip -DOCSTART- and the next line\n","            fh.readline()\n","        elif line == \"\":\n","            #Sentence ended.\n","            myoutput.append([words,tags])\n","            words,tags = [],[]\n","        else:   \n","            parts = line.split()\n","            #word, pos_tag, chunk_tag, ner_tag = line.split()\n","            if len(parts) == 4:\n","                words.append(parts[0])\n","                tags.append(parts[-1])\n","    fh.close()\n","    return myoutput"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_4_mPw5ZBRm"},"source":["## Extracting features\n","\n","The code below presents a function (`sent2feats`) that extracts the features of a given setences. That is, given a sentence (a list of tokens) and extracts the set of features related to each token (a list of dictionaries, where each dictionary represents a feature for that word). \n","\n","As you can see, the functions only extracts two features per token:\n","\n"," - `wordsfeats['word'] = word`: the word form of the current token\n"," - `wordsfeats['tag'] = tag`: the pos tag of the current token\n","\n","\n","You can first evaluate the current feautes function to see how informative is to model the token with just the word form and its part of speech tag. \n","\n","Then as an exercise you can define and extracts a larger set of features. For example you can extract as feature set the following:\n","\n"," - __Word features__: word, prev 2 words, next 2 words in the sentence.\n"," - __POS tag features__: current tag, previous and next 2 tags.\n"," - __Word shape features__: If current word is all caps, word shape, next 2 words shape\n"," - __Gazetteers__: The presence of current word in a gazetteers. Geonames can be a good resource for this task: https://www.geonames.org/ Using the whole dataset might be too tedious as the file is quite large, so for this lab might be enough to use the list of cities with a population bigger than 15000. \n","   - Check the readme of geonames files: http://download.geonames.org/export/dump/readme.txt\n","   - All files can be downloaded from here: http://download.geonames.org/export/dump/\n","\n","At least you need to implement the first two feature set to obtain competitive results, **but first continue with the current default features until the end of the notebook**.\n"]},{"cell_type":"code","metadata":{"id":"wxIFwVSDZIUz"},"source":["\"\"\"\n","Get features for all words in the sentence\n","Features:\n","- word context: a window of 2 words on either side of the current word, and current word.\n","- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n","input: sentence as a list of tokens.\n","output: list of dictionaries. each dict represents features for that word.\n","\"\"\"\n","def sent2feats(sentence):\n","    feats = []\n","    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n","    for i in range(0,len(sentence)):\n","        word = sentence[i]\n","        wordfeats = {}\n","       #word features: word, prev 2 words, next 2 words in the sentence.\n","        wordfeats['word'] = word\n","        if i == 0:\n","            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n","        elif i==1:\n","            wordfeats[\"prevWord\"] = sentence[0]\n","            wordfeats[\"prevSecondWord\"] = \"</S>\"\n","        else:\n","            wordfeats[\"prevWord\"] = sentence[i-1]\n","            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n","        #next two words as features\n","        if i == len(sentence)-2:\n","            wordfeats[\"nextWord\"] = sentence[i+1]\n","            wordfeats[\"nextNextWord\"] = \"</S>\"\n","        elif i==len(sentence)-1:\n","            wordfeats[\"nextWord\"] = \"</S>\"\n","            wordfeats[\"nextNextWord\"] = \"</S>\"\n","        else:\n","            wordfeats[\"nextWord\"] = sentence[i+1]\n","            wordfeats[\"nextNextWord\"] = sentence[i+2]\n","        \n","        #POS tag features: current tag, previous and next 2 tags.\n","        wordfeats['tag'] = sen_tags[i][1]\n","        if i == 0:\n","            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n","        elif i == 1:\n","            wordfeats[\"prevTag\"] = sen_tags[0][1]\n","            wordfeats[\"prevSecondTag\"] = \"</S>\"\n","        else:\n","            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n","            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n","            # next two words as features\n","        if i == len(sentence) - 2:\n","            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n","            wordfeats[\"nextNextTag\"] = \"</S>\"\n","        elif i == len(sentence) - 1:\n","            wordfeats[\"nextTag\"] = \"</S>\"\n","            wordfeats[\"nextNextTag\"] = \"</S>\"\n","        else:\n","            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n","            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n","        \n","        # WordShape: current wordShape\n","        wordfeats['wordShape'] = wordshape(word)\n","        #That is it! You can add whatever you want!\n","        feats.append(wordfeats)\n","    return feats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkAxZ43-uqVF"},"source":["def wordshape(text):\n","    import re\n","    t1 = re.sub('[A-Z]', 'X',text)\n","    t2 = re.sub('[a-z]', 'x', t1)\n","    return re.sub('[0-9]', 'd', t2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_vs1QkcZojM"},"source":["#Extract features from the conll data, after loading it.\n","def get_feats_conll(conll_data):\n","    feats = []\n","    labels = []\n","    for sentence in conll_data:\n","        feats.append(sent2feats(sentence[0]))\n","        labels.append(sentence[1])\n","    return feats, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFGxiHaxg_Zn"},"source":["# input example\n","example = [[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']]]\n","\n","feats, labels = get_feats_conll(example)\n","print(example)\n","feats[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXh1ejlBZA8q"},"source":["## Training the model"]},{"cell_type":"code","metadata":{"id":"WLoCTCXdZt6R"},"source":["#Train a sequence model\n","def train_seq(X_train,Y_train,X_dev,Y_dev):\n","    crf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)#, all_possible_states=True)\n","    #Just to fit on training data\n","    crf.fit(X_train, Y_train)\n","    labels = list(crf.classes_)\n","\n","    #testing:\n","    y_pred = crf.predict(X_dev)\n","    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n","    print(metrics.flat_f1_score(Y_dev, y_pred,average='weighted', labels=labels))\n","    print(metrics.flat_classification_report(Y_dev, y_pred, labels=sorted_labels, digits=3))\n","    get_confusion_matrix(Y_dev, y_pred,labels=sorted_labels)\n","    return crf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dL-6mC6tfqbR"},"source":["def print_cm(cm, labels):\n","    print(\"\\n\")\n","    \"\"\"pretty print for confusion matrixes\"\"\"\n","    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n","    empty_cell = \" \" * columnwidth\n","    # Print header\n","    print(\"    \" + empty_cell, end=\" \")\n","    for label in labels:\n","        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n","    print()\n","    # Print rows\n","    for i, label1 in enumerate(labels):\n","        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n","        sum = 0\n","        for j in range(len(labels)):\n","            cell = \"%{0}.0f\".format(columnwidth) % cm[i, j]\n","            sum =  sum + int(cell)\n","            print(cell, end=\" \")\n","        print(sum) #Prints the total number of instances per cat at the end."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEtBtfYrfr6o"},"source":["#python-crfsuite does not have a confusion matrix function, \n","#so writing it using sklearn's confusion matrix and print_cm from github\n","def get_confusion_matrix(y_true,y_pred,labels):\n","    trues,preds = [], []\n","    for yseq_true, yseq_pred in zip(y_true, y_pred):\n","        trues.extend(yseq_true)\n","        preds.extend(yseq_pred)\n","    print_cm(confusion_matrix(trues,preds,labels=labels),labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNWnYq471FmU"},"source":["Make sure your path is correct!"]},{"cell_type":"code","metadata":{"id":"k9_-Efy4az7e"},"source":["# main script\n","work_dir = \"drive/MyDrive/Colab Notebooks/nlp-app-II/data\"\n","conll_dir = work_dir + \"/conll2003/en\"\n","train_path = conll_dir + \"/train.txt\"\n","dev_path = conll_dir + \"/valid.txt\"\n","test_path = conll_dir + \"/test.txt\"\n","\n","conll_train = load_data_conll(train_path)\n","conll_dev = load_data_conll(dev_path)\n","\n","print(\"Training a Sequence classification model with CRF\")\n","feats, labels = get_feats_conll(conll_train)\n","devfeats, devlabels = get_feats_conll(conll_dev)\n","crf_model = train_seq(feats, labels, devfeats, devlabels)\n","print(\"Done with sequence model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K-3J_iq4v8nb"},"source":["## Exercise 1\n","Train first the CRF model with the feature sets provided by default (that is, current word and part-of-speech), and analyze the results. Why are we getting such a high overall results? Are they actually good results? What is happening there?\n","\n","It is always a good idea to the use official scorer when possible.\n"]},{"cell_type":"code","metadata":{"id":"Lg7PrmAJ5bpJ"},"source":["# Note we are using dev partition as test. \n","# Change to \"text.txt\" when you want to evaluate on test (do it once you are happy in dev)\n","test_path = conll_dir + \"/valid.txt\"\n","\n","conll_test = load_data_conll(test_path)\n","testfeats, testlabels = get_feats_conll(conll_test)\n","words = [tokens[0] for tokens in conll_test]\n","\n","preds = crf_model.predict(testfeats)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2xvcUf55-u0"},"source":["def dump_to_file(words, labels, preds):\n","    f = open('output.tsv', 'w', encoding='utf-8')\n","    for i in range(len(preds)):\n","        for w, l,p in zip(words[i],labels[i], preds[i]):\n","            f.write(w + \" \" + l + \" \" + p + \"\\n\")\n","        f.write('\\n')\n","    f.close()\n","\n","dump_to_file(words, testlabels, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEQV5QytArsq"},"source":["!cp \"drive/MyDrive/00-Irakaskuntza/HAP-LAP-masterra/NLP-Applications-2/Part1: Information-extraction/notebooks/conlleval.txt\" ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdNSFx3J8Lb5"},"source":["!perl conlleval.txt < output.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gTSy95_w5Ua"},"source":["## Exercise 2\n","\n","Look at the confusion matrix. Note that, rows are gold labels and colums are the predicted labels. Where is located the confusion? "]},{"cell_type":"markdown","metadata":{"id":"396Se1qMamC7"},"source":["## Exercise 3\n","Build you own feature set. You can extract as feature set the following:\n","\n"," - __Word features__: word, prev 2 words, next 2 words in the sentence.\n"," - __POS tag features__: current tag, previous and next 2 tags.\n"," - __Word shape features__: If current word is all caps, word shape, next 2 words shape. We provide a function to get the word shape. \n"," - __Gazetteers__: The presence of current word in a gazetteers.\n","\n","In order to onbtain some competitive results (and understand how to build a function to extract features for sequence labeling) you should implement the first two sets of features (word features, and pos tag features)."]}]}