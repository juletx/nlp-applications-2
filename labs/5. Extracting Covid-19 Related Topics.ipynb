{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. Extracting Covid-19 Related Topics.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kcqhNCf41SMG"},"source":["# Topic Modelling COVID-19 Dataset\n","\n","**Table of Contents**\n","\n","<a href='#section1'> 1. Introduction</a>\n","\n","<a href='#section2'> 2. Data Loading and Exploration</a>\n","\n","<a href='#section3'> 3. Summary Statistics of Corpus</a>\n","\n","<a href='#section4'> 4. Preprocessing: Tokenization, Lemmatization and Stopwords</a>\n","\n","<a href='#section5'> 5. Topic Model: Fitting Latent Dirichlet Allocation Model</a>\n","\n","<a href='#section6'> 6. Inspection of the topics</a>\n","\n","><a href='#section6.1'>6.1. Visualization of topics</a>\n","\n","><a href='#section6.2'>6.2 Word Similarity</a>\n","\n","><a href='#section6.3'>6.3 Plotting words in 2D</a>\n","\n","<a href='#section7'> 7. Document Similarity</a>\n","\n","><a href='#section7.1'> 7.1. Ploting Documents in 2D</a>\n","\n","<a href='#section8'> 8. Model Performance</a>\n","\n","<a href='#section9'> 9. Document Clustering</a>\n","\n","----\n","\n","**Exercise list**\n","\n","- [Ex1: Important information](#ex1)\n","\n","- <a href='#ex2'>Ex2: Text vectorization</a>\n","\n","- <a href='#ex3'>Ex3: Topic inspection</a>\n","\n","- <a href='#ex4'>Ex4: Document similarity</a>\n","\n","- <a href='#ex5'>Ex5: Model performance</a>\n","\n","- <a href='#ex6'>Ex6: Document clustering</a>\n","\n","-----\n","\n","<a id='section1'></a>\n","## 1. Introduction\n","Topic models allow to discover (induce) the topics in a given set of documents. It can be understood as a probabilistic clustering method that groups words belonging to the same topic, associated with a probability. We assume that documents exhibit multiple topics (a multinomial distribution over topics), and each topic is a distribution over words (defined by a multinomial distribution over words). \n","\n","In this assignment we will estimate emergins topics of a corpus of biomedicine domain. We will be using a part of the dataset of [COVID-19 Open Research Dataset Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge). The CORD-19 dataset contains metadata of over 51,000 scientific papers (full text is also available for around 40,000 of them) about COVID-19, SARS-CoV-2, and related coronaviruses. In this notebook we will be modeling topics for paper published between 2019 and 2020. \n","\n","\n",">>![](https://drive.google.com/uc?id=1fUETDUs3vMLvbe9xWq1Wr-fuimjpf51_)\n"]},{"cell_type":"markdown","metadata":{"id":"NEWxMgm41wxn"},"source":["**Set up**\n","\n","Please, run the following cells to mount your Drive to your Colaboratory session, and enable plot.ly charts as well."]},{"cell_type":"code","metadata":{"id":"Br0YI4ZLxreW"},"source":["!pip install pyldavis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTZim3L51obo"},"source":["# Mount Drive files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9pYQcCJ81rRG"},"source":["def enable_plotly_in_cell():\n","  import IPython\n","  from plotly.offline import init_notebook_mode\n","  display(IPython.core.display.HTML('''\n","        <script src=\"/static/components/requirejs/require.js\"></script>\n","  '''))\n","  init_notebook_mode(connected=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ViEcJWg1SMI"},"source":["<a id='section2'></a>\n","## 2. Load data and Exploration\n","We first will load the corpus with Pandas as a DataFrame object. Data-frame object has many columns, but text is stored in `abstract`.\n","\n","__Note__: Make sure your path in \"data_path\" is correct. Otherwise you won't be able to run the notebook."]},{"cell_type":"code","metadata":{"id":"VzBKShv01SMJ"},"source":["import pandas as pd\n","import numpy as np\n","from scipy import stats\n","\n","\n","# Plotly imports\n","import plotly.offline as py\n","py.init_notebook_mode(connected=True)\n","import plotly.graph_objs as go\n","import plotly.tools as tls\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","\n","def load_metadata(path_to_metada, below_year=2020, above_year=1950):\n","    # Select interesting fields from metadata file\n","    fields = ['cord_uid', 'title', 'authors', 'publish_time', 'abstract', 'journal', 'url']\n","    # Extract selected fields from metadata file into dataframe\n","    df_mdata = pd.read_csv(path_to_metada, skipinitialspace=True, index_col='cord_uid', usecols=fields)\n","\n","    # WARNING: cord_uid is described as unique, but c4u0gxp5 is repeated. So I remove one of this\n","    df_mdata = df_mdata.loc[~df_mdata.index.duplicated(keep='first')]\n","    df_mdata['publish_time'] = pd.to_datetime(df_mdata['publish_time'], errors=\"coerce\")\n","    df_mdata['publish_year'] = df_mdata['publish_time'].dt.year\n","    df_mdata = df_mdata[df_mdata['abstract'].notna()]\n","    df_mdata = df_mdata[df_mdata['authors'].notna()]\n","    # df_mdata = df_mdata[df_mdata['sha'].notna()]\n","    df_mdata['authors'] = df_mdata['authors'].apply(lambda row: str(row).split('; '))\n","\n","    relevant_time = df_mdata.publish_year.between(above_year, below_year)\n","    df_mdata = df_mdata[relevant_time]\n","\n","    return df_mdata[['title', 'authors', 'publish_time', 'abstract', 'journal', 'url']]\n","\n","\n","data_path='drive/MyDrive/Colab Notebooks/nlp-app-II/data/'\n","df = load_metadata(data_path+'metadata.csv', below_year=2020, above_year=2019)\n","\n","#df['doc_name'] = df['doc_name'].astype(str)\n","#df['text'] = df['text'].astype(str)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yFvqIiX1SMP"},"source":["<a id='section3'></a>\n","## Summary statistics of the COVID-19 corpus\n","\n","Here we can visualize some basic statistics in the data, like the distribution of the document-length of the article that comprised the corpus. For this purpose, we will use Plot.ly visualisation library and plot some simple bar plots.\n","\n","Following code approximately counts the length of the documents in words. Note that we are spliting the words by whitespace, and this is not the most accurate way of tokenization."]},{"cell_type":"code","metadata":{"id":"SviziCw21SMQ"},"source":["enable_plotly_in_cell()\n","\n","lengths = np.asarray(df['abstract'].apply(lambda sent: len(sent.split(' '))))\n","\n","print('Mean length: {}'.format(lengths.mean()))\n","print('Most repeated length: {}'.format(stats.mode(lengths)))\n","\n","data = [go.Histogram(x=lengths)]\n","\n","layout = go.Layout(\n","    title='Document length frequencies',\n","    xaxis=dict(\n","        title='Abstract length [words]'\n","    ),\n","    yaxis=dict(\n","        title='Count'\n","    ),\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-bar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDmIZw0zDjjA"},"source":["**Number of documents**\n","\n","Following we will calculate the size of the dataset in terms of number of abstract. Note that when loading the information in the metadata file we only take into account the paper published between 2019 and 2020. "]},{"cell_type":"code","metadata":{"id":"NjucIb1vDYen"},"source":["print(\"Number of abstracts loaded: {} \".format(df.shape[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XiIP3Z44Fc1J"},"source":["**Word frequencies**\n","\n","\n","Word frequencies can tell a lot about the corpus we are working with, but it is not completely straightforward. \n"]},{"cell_type":"code","metadata":{"id":"kS0VXMYW1SMY"},"source":["enable_plotly_in_cell()\n","\n","all_words = df['abstract'].str.split(expand=True).unstack().value_counts()\n","data = [go.Bar(\n","            x = all_words.index.values[2:50],\n","            y = all_words.values[2:50],\n","            marker= dict(colorscale='Jet',\n","                         color = all_words.values[2:100]\n","                        ),\n","            text='Word counts'\n","    )]\n","\n","layout = go.Layout(\n","    title='Top 50 (Uncleaned) Word frequencies in the corpus'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-bar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcjcrix21SMc"},"source":["Do you see anything odd about the words that appear in this word frequency plot? Do these words actually tell us much about the themes and concepts that should contain a corpus of biomedicine? \n","\n","These words are all so commonly occuring words which you could find just anywhere else. Therefore we must find some way to preprocess our dataset first to strip out all these commonly occurring words which do not bring much to the table.\n","\n","#### Exercise 1 <a class=\"anchor\" id=\"ex1\"></a>\n","\n","- How would you surface/extract the most important features for modeling the topics?"]},{"cell_type":"markdown","metadata":{"id":"DBiR8AZH1SMd"},"source":["<a id='section4'></a>\n","\n","## 4. Preprocessing: Tokenization, lemmatization, removing semantically empty stuff"]},{"cell_type":"markdown","metadata":{"id":"yB1khYZK1SMe"},"source":["In almost all Natural Language Processing tasks that you will come across, one will generally always have to undergo these few pre-processing steps to convert the input raw text into a form that is readable by your model and the machine. Text pre-processing can be boiled down to these few simple steps:\n","\n","1. **Tokenization** - Segmentation of the text into its individual constitutent words. \n","2. **Stopwords** - Throw away any words that occur too frequently as its frequency of occurrence will not be useful in helping detecting relevant texts. (as an aside also consider throwing away words that occur very infrequently).\n","3. **Vectorization** - Converting text into vector format. One of the simplest is the famous bag-of-words approach, where you create a matrix (for each document or text in the corpus). In the simplest form, this matrix stores word frequencies (word counts) and is oft referred to as vectorization of the raw text. \n","\n","There are many toolkits in Python that help preprocessing input text. Two well-known packages are: \n","\n","- [**Natural Language Toolkit (NLTK)**](http://www.nltk.org/) \n","\n","- [**SpaCy**](https://spacy.io/)\n","\n","We will use SpaCy toolkit in this notebook, but the use of NLTK should be similar."]},{"cell_type":"code","metadata":{"id":"WAcGwtWg1SMf"},"source":["import spacy\n","from spacy.lang.en.examples import sentences \n","import string\n","\n","def preprocess(data, remove_stopwords=True, remove_func_words=True):    \n","    nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","    if remove_func_words:\n","        open_class_words = set(['NOUN', 'ADV', 'VERB','ADJ'])\n","        data['preproc'] = data['abstract'].apply(lambda row: [tok.lemma_.lower() for tok in nlp(row) if tok.pos_ in open_class_words])\n","    else:\n","        data['preproc'] = data['abstrac'].apply(lambda row: [tok.lemma_.lower() for tok in nlp(row)])\n","    \n","    if remove_stopwords:\n","        stop = nlp.Defaults.stop_words\n","        data['preproc'] = data['preproc'].apply(lambda row: [word for word in row if word not in stop])\n","        data['preproc'] = data['preproc'].apply(lambda row: [word for word in row if not all([c in string.punctuation for c in word])])\n","        extended_puntcation = '…–—«»'\n","        data['preproc'] = data['preproc'].apply(lambda row: [word for word in row if not all([c in extended_puntcation for c in word])])\n","        \n","    data['preproc'] = data['preproc'].apply(lambda row: ' '.join(row))\n","    return data\n","\n","df = preprocess(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RoxI_Qj-1SMh"},"source":["'preprocess' function adds new columns in the dataframe: 'preproc'. Preproc columns contains tokenize and cleaned documents.\n","\n","- Spacy tokenization\n","- Spacy part-of-speech tagging to get only content words.\n","- Remove stopwords.\n","- Remove punctuation marks."]},{"cell_type":"code","metadata":{"id":"AAKQgJz71SMi"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZKXEkJ91SMm"},"source":["**Revisiting term frequencies**\n","\n","Having implemented our lemmatized count vectorizer, let us revist the plots for the term frquencies of the top 50 words (by frequency). As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot "]},{"cell_type":"code","metadata":{"id":"DEtYdo0W1SMn"},"source":["enable_plotly_in_cell()\n","\n","# word frequencies\n","all_words = df['preproc'].str.split(expand=True).unstack().value_counts()\n","\n","data = [go.Bar(\n","            x = all_words.index.values[0:50],\n","            y = all_words.values[0:50],\n","            marker= dict(colorscale='Jet',\n","                         color = all_words.values[2:100]\n","                        ),\n","            text='Word counts'\n","    )]\n","\n","layout = go.Layout(\n","    title='Top 50 (cleaned) Word frequencies in the training dataset'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-bar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrPIB-Pi1SMr"},"source":["Plot clearly shows which is the domain of the corpus. Now as the most representative we have words such as 'cell', 'gene', or 'treatment'. But still we have some general words like 'use', 'find' and 'common', among others.  "]},{"cell_type":"markdown","metadata":{"id":"tmBwG9SV1SMs"},"source":["<a id='section5'></a>\n","\n","## 5. Topic Model: Fitting Latent Dirichlet Allocation Model\n","\n","Now we are ready to stimate the topics from text using the [Latent Dirichlet Allocation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) algorithm . I will be using Sklearn's implementation. Another very well-known LDA implementation is Radim Rehurek's [gensim](https://radimrehurek.com/gensim/), so check it out as well."]},{"cell_type":"markdown","metadata":{"id":"bFhI2EOm1SMs"},"source":["**Corpus - Document - Word : Topic Generation**\n","\n","In LDA, the modelling process revolves around three things: the text corpus, its collection of documents, D and the words W in the documents. Therefore the algorithm attempts to uncover K topics from this corpus via the following way (illustrated by the diagram)\n","\n","![Three_Level Bayesian Model](http://scikit-learn.org/stable/_images/lda_model_graph.png)\n","\n","Model each topic, $\\kappa$ via a Dirichlet prior distribution given by $\\beta_{k}$:\n","\n","![](http://scikit-learn.org/stable/_images/math/2c1ff5b3d6f342d7dad0395210c8a13947de451c.png)\n","\n","Model each document d by another Dirichlet distribution parameterized by $\\alpha$:\n","\n","![](http://scikit-learn.org/stable/_images/math/530c80986933767c9d182af83075c13d72cdef97.png)\n","\n","Subsequently for document d, we generate a topic via a multinomial distribution which we then backtrack and use to generate the correspondings words related to that topic via another multinomial distribution:\n","\n","![](http://scikit-learn.org/stable/_images/math/0bb078c0fe621366a147231b5c8240efacdb895b.png)\n","![](http://scikit-learn.org/stable/_images/math/f7960e06327c64a1e9da6769e68aa4342b7de73d.png)\n","\n","\n","*(Image source: http://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation)*\n","\n","The LDA algorithm first models documents via a mixture model of topics. From these topics, words are then assigned weights based on the probability distribution of these topics. It is this probabilistic assignment over words that allow a user of LDA to say how likely a particular word falls into a topic. Subsequently from the collection of words assigned to a particular topic, are we thus able to gain an insight as to what that topic may actually represent from a lexical point of view.\n","\n","From a standard LDA model, there are really a few key parameters that we have to keep in mind and consider programmatically tuning before we invoke the model:\n","1. n_components: The number of topics that you specify to the model\n","2. $\\alpha$ parameter: This is the dirichlet parameter that can be linked to the document topic prior \n","3. $\\beta$ parameter: This is the dirichlet parameter linked to the topic word prior\n","\n","To invoke the  algorithm, we simply create an LDA instance through the Sklearn's *LatentDirichletAllocation* function. The various parameters would ideally have been obtained through some sort of validation scheme. In this instance, the optimal value of n_components (or topic number) was found by conducting a KMeans + Latent Semantic Analysis Scheme (as shown in this paper here) whereby the number of Kmeans clusters and number of LSA dimensions were iterated through and the best silhouette mean score."]},{"cell_type":"markdown","metadata":{"id":"jvh65E2_1SMt"},"source":["<a id='section4.1'></a>\n","### 4.1 Vectorizing Text\n","Text vectorization is basic tool that converts text into numbers that machines understand. There different ways to convert text into vector. Scikit-learn offers various functions to do this (https://scikit-learn.org/stable/modules/feature_extraction.html). We will explore few of them:\n","\n","- __Bag-of-Words__: [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n","\n","- __TF-IDF__: [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n"]},{"cell_type":"markdown","metadata":{"id":"b2_fyul91SMu"},"source":["**Example of count vectorizer**\n","\n","There are multiple ways to vectorize tokens. The example below shows that bag-of-words approach, in which each word in the vocabulary is indexed and counted how many time occurs. This representation is very useful in models like Topic Models, where the frequency of the words is very important to know.\n","\n","Below, we show the vectorization of twe sequences of words. Note that in the first sentence 'eat' is repeated three times. "]},{"cell_type":"code","metadata":{"id":"fec_guKz1SMv"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# Defining our sentence\n","sentence = [\"I love to eat eat and eat Burgers\", \n","            \"I love to eat Fries\"]\n","vectorizer = CountVectorizer(min_df=0)\n","sentence_transform = vectorizer.fit_transform(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DKrA8YwM1SMy"},"source":["We initialize and create a simple term frequency object via the CountVectorizer function simply called \"vectorizer\". The parameters that we have provided explicitly are the \"min_df\" in the parameter refers to the minimum document frequency (the rest are left as default). The vectorizer will simply drop all words that occur less than that value set.  For a detailed read up on this method as well as the rest of the parameters that one could use, I refer you to the [Sklearn website](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n","\n","Finally we apply the fit_transform method is actually comprised of two steps. The first step is the fit method where the vectorizer is mapped to the the dataset that you provide. Once this is done,  the actual vectorizing operation is performed via the transform method where the raw text is turned into its vector form as shown below\n"]},{"cell_type":"code","metadata":{"id":"vK0Z_LTu1SMz"},"source":["print(\"The features are:\\n {}\".format(vectorizer.get_feature_names_out()))\n","print(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1oo8dpk1SM4"},"source":["**Vectorizing the whole dataset**\n","\n","Combining Pandas and Scikit-learn makes very easy this part of the precossesing. First, we create the vectorizer object of sklearn, and then we can convert to vectors our tokenized and cleaned corpus directly with fit_transform method."]},{"cell_type":"code","metadata":{"id":"rvnXC6Oy1SM5"},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# the vectorizer object will be used to transform text to vector form\n","vectorizer = CountVectorizer(strip_accents = 'unicode',\n","                             stop_words = 'english',\n","                             lowercase = True,\n","                             token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n","                             max_df = 0.5, \n","                             min_df = 10)\n","\n","# apply transformation\n","tf = vectorizer.fit_transform(df['preproc'])\n","\n","# tf_feature_names is the word index of the vectorizer\n","tf_feature_names = vectorizer.get_feature_names_out()\n","print(tf_feature_names[0:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9s7l4PpJ1SM8"},"source":["<a id='ex3'></a>\n","#### Exercise 2\n","You can apply different options when vectorizating the input. For example, you can try using  different thresholds (min_df, max_df) and see what is the difference."]},{"cell_type":"markdown","metadata":{"id":"E8yeVZb-1SM9"},"source":["### 5.1 Fitting topics"]},{"cell_type":"code","metadata":{"id":"Y4fBbU7B1SM-"},"source":["from sklearn.decomposition import LatentDirichletAllocation\n","number_of_topics = 100\n","model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdx1Ch411SND"},"source":["model.fit(tf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t0OYKpqG1SNK"},"source":["<a id='section6'></a>\n","\n","## 6. Inspection of the topics"]},{"cell_type":"code","metadata":{"id":"FHk_twU_1SNL"},"source":["def display_topics(model, feature_names, no_top_words):\n","    topic_dict = {}\n","    for topic_idx, topic in enumerate(model.components_):\n","        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n","                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n","    return pd.DataFrame(topic_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiBWgqhk1SNN"},"source":["no_top_words= 10\n","display_topics(model, tf_feature_names, no_top_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYaA3K861SNQ"},"source":["<a id='section 6.1'></a>\n","### 6.1. Visualization of topics"]},{"cell_type":"code","metadata":{"id":"Jld4ZSVR1SNR"},"source":["# pip install pyldavis\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","pyLDAvis.enable_notebook()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vZcfucu1SNT"},"source":["pyLDAvis.sklearn.prepare(model, tf, vectorizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17bMjWcj1SNW"},"source":["#### Exercise 3 <a class=\"anchor\" id=\"ex3\"></a>\n","- Does estimated topics make sense? Could you name some of them?"]},{"cell_type":"markdown","metadata":{"id":"fgWpsyBt1SNX"},"source":["<a id='section6.2'></a>\n","### 6.2 Word similarity"]},{"cell_type":"code","metadata":{"id":"Avk98Mo71SNX"},"source":["from sklearn.metrics.pairwise import cosine_distances\n","\n","def most_probable_words(topic_word_vector, word_topic_table, vocab, top_n=10):\n","  sim = topic_word_vector.dot(word_topic_table)\n","  knn = np.argsort(-sim)\n","  similar_words = [vocab[i] for i in knn[0:top_n]]\n","  return similar_words\n","\n","def most_similar_words(word_vector, word_table, vocab, top_n=10):\n","  dists = cosine_distances(word_vector.reshape(1, -1), word_table)\n","  pairs = enumerate(dists[0])\n","  most_similar = sorted(pairs, key=lambda item: item[1])[:top_n]\n","  similar_words = [vocab[i[0]] for i in most_similar]\n","  return similar_words\n","\n","# P(t | w): topic distr given word w\n","topic_prob = np.transpose(model.components_) # P(t | w)\n","row_sums = np.sum(topic_prob, axis=1)\n","topic_prob = topic_prob / row_sums[:, np.newaxis]\n","\n","# P(w | t): word probability given topic t\n","word_prob = model.components_ # P(w | t)\n","row_sums = np.sum(word_prob, axis=1)\n","word_prob = word_prob / row_sums[:, np.newaxis]\n","\n","# word indices\n","word2idx = {w : i for i, w in enumerate(tf_feature_names)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Achqwr5Z0kT"},"source":["Change the variable `word` to obtain more similar words"]},{"cell_type":"code","metadata":{"id":"SV-y74GoZz0l"},"source":["word = 'virus'\n","\n","# P(w2 | w1)\n","topic_word_vector = topic_prob[word2idx[word]]  # p(t | w)\n","similar_words = most_probable_words(topic_word_vector, word_prob, tf_feature_names)\n","print('P(w2 | w1)\\nMost probable words to \"{}\":'.format(word))\n","print(similar_words)\n","\n","print(\"\")\n","\n","# cos(p(t|w1), p(t|w2))\n","similar_words = most_similar_words(topic_word_vector, topic_prob, tf_feature_names)\n","print('cos(p(t|w1), p(t|w2))\\nMost similar words to \"{}\":'.format(word))\n","print(similar_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39BnftdS1SNa"},"source":["<a id='section6.3'></a>\n","### 6.3 Ploting words in 2D"]},{"cell_type":"code","metadata":{"id":"SC-P7pyY1SNb"},"source":["from sklearn.manifold import TSNE\n","\n","tsne = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n","words_2d = tsne.fit_transform(topic_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9cn9kM71SNg"},"source":["enable_plotly_in_cell()\n","\n","df_2d = pd.DataFrame(columns=['x', 'y', 'word'])\n","df_2d['x'], df_2d['y'], df_2d['word'] = words_2d[:,0], words_2d[:,1], np.asarray(tf_feature_names)\n","\n","df_2d = df_2d.sample(n=200)\n","\n","data = [go.Scatter(\n","            x = df_2d['x'],\n","            y = df_2d['y'],\n","            mode = 'markers',\n","            marker = dict(\n","                size = 10,\n","                color = 'rgba(255, 182, 193, .9)',\n","                line = dict(\n","                    width = 2,\n","                )\n","            ),\n","            text=df_2d['word']\n","    )]\n","\n","layout = go.Layout(\n","    title='2D visualization of words'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-scatter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Urf2efg1SNm"},"source":["<a id='section7'></a>\n","## 7. Document Similarity\n","\n","Note that we can represent the documents according to their topic proportions. This latent representation can be seen as generalization over occurring words, in which documents with different words and similar topics can be considered as similar ones. "]},{"cell_type":"code","metadata":{"id":"SQNERMms1SNn"},"source":["doc_name = [\"unseen document\"]\n","text = str(df['abstract'].iloc[20])\n","title = str(df['title'].iloc[20])\n","print('Title: {}'.format(title))\n","print(text)\n","new = pd.DataFrame({'doc_name':doc_name, 'abstract':text})\n","new = preprocess(new)\n","x_new = model.transform(vectorizer.transform(new['abstract']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnzXKrbi1SNt"},"source":["from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n"," \n","def most_similar(x, Z, top_n=5):\n","    dists = cosine_distances(x.reshape(1, -1), Z)\n","    pairs = enumerate(dists[0])\n","    most_similar = sorted(pairs, key=lambda item: item[1])[:top_n]\n","    return most_similar\n","  \n","# get latent representaiton of documents \n","tf_Z = model.transform(tf)\n","\n","# get most similar documents\n","similarities = most_similar(x_new, tf_Z)\n","document_id, similarity = similarities[0]\n","\n","similar_ids = [sim[0] for sim in similarities]\n","\n","# print most similar document\n","df.iloc[similar_ids]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-wzaOOQX1SN2"},"source":["\n","#### Exercise 4 <a class=\"anchor\" id=\"ex4\"></a>\n","\n","- Select random article from the dataset and predict the topics of the unseen document. \n","- Do not forget to preprocess the new document!"]},{"cell_type":"markdown","metadata":{"id":"cW3_yZxi1SN3"},"source":["<a id='section7.1'></a>\n","### 7.1 Ploting documents in 2D"]},{"cell_type":"code","metadata":{"id":"EmbADRgS7tVV"},"source":["from sklearn.manifold import TSNE\n","\n","tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n","tsne_lda = tsne_model.fit_transform(tf_Z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZC6E6wO78Pxl"},"source":["df_2d = pd.DataFrame(columns=['x', 'y', 'document'])\n","df_2d['x'], df_2d['y'], df_2d['document'] = tsne_lda[:,0], tsne_lda[:,1], np.asarray(df['title'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4OTWyZO1SN7"},"source":["enable_plotly_in_cell()\n","\n","data = [go.Scatter(\n","            x = df_2d['x'],\n","            y = df_2d['y'],\n","            mode = 'markers',\n","            marker = dict(\n","                size = 10,\n","                color = 'rgba(255, 182, 193, .9)',\n","                line = dict(\n","                    width = 2,\n","                )\n","            ),\n","            text=df_2d['document']\n","    )]\n","\n","layout = go.Layout(\n","    title='2D visualization of documents'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-scatter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oUkWCsH1SOD"},"source":["<a id='section8'></a>\n","## 8. Diagnose Model Performance\n","Measure the performance of topic models is challenging, as there is no easy way to measure how coherent are the estimated topics. One correct way to do it is to evaluated extrinsically, in which we apply the model in a final task like information retrieval and similar. This way to evaluate is usually quite expensive. Therefore, the most common way to evaluate such models is by measuring the log-likehood or perplexity of the model for a given unseen set of documents. \n","\n","Two measure are mathematically related, but whereas we find higher log-likehood values, we want a lower perplexity value. Intuitively, the metrics measure how well the model is fitted to the data (or how likely is to generate the given data). "]},{"cell_type":"code","metadata":{"id":"DhulbMQx1SOE"},"source":["from pprint import pprint\n","\n","# Log Likelyhood: Higher the better\n","print(\"Log Likelihood: \", model.score(tf))\n","\n","# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n","print(\"Perplexity: \", model.perplexity(tf))\n","\n","# See model parameters\n","pprint(model.get_params())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFNI3ReYHVKm"},"source":["#### Exercise 5 <a class=\"anchor\" id=\"ex5\"></a>\n","\n","- Try to find the best number of topics that get low perplexity. This can take a good amount of time, so do not go for a very exhaustive search."]},{"cell_type":"markdown","metadata":{"id":"JkAz7sxsHymm"},"source":["<a id='section9'></a>\n","## 9. Clustering documents\n","Topic models can be seen as latent representation of documents, in which we make visible the main topic/themes of the document. As we seen above, the representation can useful to find similar documents (and words). Having a way to represent documents and measure their similarity we can easily set up a clustering algorithm that group documents by their topics. "]},{"cell_type":"code","metadata":{"id":"DORBVIw5H4K2"},"source":["# Construct the k-means clusters\n","from sklearn.cluster import KMeans\n","clusters = KMeans(n_clusters=15, random_state=100).fit_predict(tf_Z)\n","\n","clusters.shape\n","\n","# Build the Singular Value Decomposition(SVD) model\n","tsne_model = TSNE(n_components=2, verbose=0, random_state=0, angle=.99, init='pca')  # 2 components \n","lda_output_tsne = tsne_model.fit_transform(tf_Z)\n","\n","# X and Y axes of the plot using TSNE decomposition\n","x = lda_output_tsne[:, 0]\n","y = lda_output_tsne[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpoHoDfLKg6N"},"source":["enable_plotly_in_cell()\n","\n","data = [go.Scatter(\n","            x = x,\n","            y = y,\n","            mode = 'markers',\n","            marker = dict(\n","                size = 10,\n","                color = clusters,\n","                line = dict(\n","                    width = 2,\n","                )\n","            ),\n","            text=np.asarray(df['title'])\n","    )]\n","\n","layout = go.Layout(\n","    title='Segregation of Topic Clusters'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-scatter')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBLpTJjkH8tv"},"source":["#### Exercise 6 <a class=\"anchor\" id=\"ex6\"></a>\n","- Inspect some clusters and indentify examples that make sense."]}]}