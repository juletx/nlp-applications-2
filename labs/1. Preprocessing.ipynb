{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Preprocessing.ipynb","provenance":[],"authorship_tag":"ABX9TyMpcTl5ds1tpFCWOUwGPbfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_E0EJkR-804K"},"source":["# 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"XEhfYh1D6ZUW"},"source":["In this notebook we will familiarize with some topics related to preprocessing. You will be using some existing pipelines for NLP to preprocess a dataset for sentiment analysis.  Preprocessed dataset we'll be used as input for naive heuristic based sentiment analysis. \n","\n","The dataset we are going to use ranges the polarity annotation from 0 to 5, where 0 denotes extremely negative sentiment, and 5 is the most positive.Nevertheless, for this lab we'll simplify the task, and we will translate the 5-way classification task into 2-way classification task (0  →  negative, ;1  →  positive),\n","\n","At the end of the notebook, we will be using a Python implementation for doing some easy data augmentation (EDA).\n","\n","**Goals**:\n","- To learn using some of the existing pipelines \n","  + [**Natural Language Toolkit (NLTK)**](http://www.nltk.org/) \n","  + [**SpaCy**](https://spacy.io/)\n","  + [**Stanford NLP**](https://stanfordnlp.github.io/stanfordnlp/)\n","  + [**Trankit**](http://nlp.uoregon.edu/trankit)\n","- Measure the effect of different preprocessing in specific tasks such as sentiment analysis.\n","- To learn doing some EDA\n"]},{"cell_type":"markdown","metadata":{"id":"zZbQhLJu88HL"},"source":["# 2. Load data\n","\n","Let's load the Stanford Sentiment Treebank.  The data can be originaly downloaded from here: [the train/dev/test Stanford Sentiment Treebank distribution](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip). But **you don't need to download!** If you already copied ```nlp-app-II``` folder to your ```Colab Notebooks```, you should have the data for this lab in ```nlp-app-II/data/trees```. \n","\n","In order to load the data, you we'll need to mount your Drive folder first and give the access to the Notebook. This will require one-step authentication. Please when you run the cell below follow the instructions.\n","\n","Once you mount everything, make sure ```sst_home = 'drive/My Drive/Colab Notebooks/nlp-app-II/data/trees/''``` is correct path for the data.\n"]},{"cell_type":"code","metadata":{"id":"wvcgi7JG6X9H"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLtCdeZ3HHY6"},"source":["# Load the data\n","import re\n","import pandas as pd\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    data = pd.DataFrame(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/nlp-app-II/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGRkIHj4HWtK"},"source":["training_set.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlBlu0It9Hpj"},"source":["# 3. Preprocessing: Tokenization, lemmatization, removing semantically empty stuff\n","In almost all Natural Language Processing tasks that you will come across, one will generally always have to undergo few pre-processing steps to convert the input raw text into a form that is readable by your model and the machine. Text pre-processing can be boiled down to these few simple steps:\n","\n","1. **Tokenization** - Segmentation of the text into its individual constitutent words. \n","2. **Lemmatization** - The process of mapping all the different forms of a word to its base form (_lemma_).\n","3. **PoS tagging** - The process of mapping a word to its gramatical category in the sentence.\n","3. **Stopwords** - Throw away any words that occur too frequently as its frequency of occurrence will not be useful in helping detecting relevant texts. (as an aside also consider throwing away words that occur very infrequently).\n","\n","There are many toolkits in Python that help preprocessing input text. Four well-known packages are: \n","\n","- [**Natural Language Toolkit (NLTK)**](http://www.nltk.org/) \n","- [**SpaCy**](https://spacy.io/)\n","- [**Stanford NLP**](https://stanfordnlp.github.io/stanfordnlp/)\n","- [**Trankit**](http://nlp.uoregon.edu/trankit)\n"]},{"cell_type":"markdown","metadata":{"id":"1ZunYhhm-sW0"},"source":["## Exercise 1\n","\n","Implement `preprocess` function to preprocess the examples in the pandas dataframe loaded above. The `preprocess` function will add a  new column in the input dataframe: `preproc`. Preproc column contains tokenize and cleaned sentences.\n","\n","The function has perfom the following preprocessing steps:\n","- Tokenization\n","- Part-of-speech tagging to get only content words.\n","- Remove stopwords.\n","- Remove punctuation marks (take a look to `string` package).\n"]},{"cell_type":"code","metadata":{"id":"2SDmNYLQ-oIk"},"source":["import spacy\n","from spacy.lang.en.examples import sentences \n","import string\n","\n","def preprocess(data, lemmatize=True, remove_stopwords=True, remove_func_words=True):\n","    ## YOUR CODE HERE    \n","    ## The function will add \"preproc\" column in the input dataframe data\n","    ## Preproc column contains tokenize and cleaned sentences.\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcA25Tz9Hjmz"},"source":["preproc_training = preprocess(training_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iz2_6atUWmLl"},"source":["preproc_training.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4R-fb7yA_dHG"},"source":["## Visualization of term frequencies\n","\n","Having preprocessed the input data, we can plot for the term frquencies of the top 50 words (by frequency) to compare . As you can see from the plot, all our prior preprocessing efforts have not gone to waste. With the removal of stopwords, the remaining words seem much more meaningful where you can see that all the stopwords in the earlier term frequency plot "]},{"cell_type":"code","metadata":{"id":"lJPv71aaZ_PA"},"source":["# Plotly imports\n","import plotly.offline as py\n","py.init_notebook_mode(connected=True)\n","import plotly.graph_objs as go\n","import plotly.tools as tls\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","def enable_plotly_in_cell():\n","  import IPython\n","  from plotly.offline import init_notebook_mode\n","  display(IPython.core.display.HTML('''\n","        <script src=\"/static/components/requirejs/require.js\"></script>\n","  '''))\n","  init_notebook_mode(connected=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lbPMUOKA8pk"},"source":["## Exercise 2\n","Plot different term-frequency barplots with and without preprocessing. Do you any differences? What happened?"]},{"cell_type":"code","metadata":{"id":"GLvuc6nHPWCv"},"source":["# Code to plot raw text.\n","\n","enable_plotly_in_cell()\n","\n","# word frequencies\n","all_words = preproc_training['text'].str.split(expand=True).unstack().value_counts()\n","\n","data = [go.Bar(\n","            x = all_words.index.values[0:50],\n","            y = all_words.values[0:50],\n","            marker= dict(colorscale='Jet',\n","                         color = all_words.values[2:100]\n","                        ),\n","            text='Word counts'\n","    )]\n","\n","layout = go.Layout(\n","    title='Top 50 (raw data) Word frequencies in the training dataset'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-bar')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7tKFpeRALqT"},"source":["# Code to plot preprocessed text\n"," \n","enable_plotly_in_cell()\n","\n","# word frequencies\n","all_words = preproc_training['preproc'].str.split(expand=True).unstack().value_counts()\n","\n","data = [go.Bar(\n","            x = all_words.index.values[0:50],\n","            y = all_words.values[0:50],\n","            marker= dict(colorscale='Jet',\n","                         color = all_words.values[2:100]\n","                        ),\n","            text='Word counts'\n","    )]\n","\n","layout = go.Layout(\n","    title='Top 50 (cleaned) Word frequencies in the training dataset'\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","py.iplot(fig, filename='basic-bar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGkro3XlBWe-"},"source":["# 4. Naive sentiment analysis\n","\n","The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension.\n","\n","We can extend this idea to calculate the polarity of a sentence, by aggregating the score of each word in the sentence. Your goal in this section is to use the semantic model to obtain the aggregated polarity score of the sentence. "]},{"cell_type":"markdown","metadata":{"id":"G0sZxtOmFcFs"},"source":["## Helper functions"]},{"cell_type":"code","metadata":{"id":"YWfnOk2oFH_Z"},"source":["import numpy as np\n","\n","def read(file, threshold=0, dim=50, vocabulary=None):\n","    count = 400000 if threshold <= 0 else min(threshold, 400000)\n","    words = []\n","    matrix = np.empty((count, dim)) if vocabulary is None else []\n","    for i in range(count):\n","        word, vec = file.readline().decode('utf-8').split(' ', 1)\n","        if vocabulary is None:\n","            words.append(word)\n","            matrix[i] = np.fromstring(vec, sep=' ')\n","        elif word in vocabulary:\n","            words.append(word)\n","            matrix.append(np.fromstring(vec, sep=' '))\n","    return (words, matrix) if vocabulary is None else (words, np.array(matrix))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGHE_g9QFWs_"},"source":["def length_normalize(matrix):\n","    norms = np.sqrt(np.sum(matrix**2, axis=1))\n","    norms[norms == 0] = 1\n","    return matrix / norms[:, np.newaxis]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaqJAffGFA0h"},"source":["def determine_coefficient(candidate_word, seed_pos, seed_neg):\n","    if candidate_word not in word2ind:\n","        return 0.0\n","    pos_ind = np.array([word2ind[word] for word in seed_pos])\n","    pos_mat = matrix[pos_ind]\n","\n","    neg_ind = np.array([word2ind[word] for word in seed_neg])\n","    neg_mat = matrix[neg_ind]\n","\n","    i = word2ind[candidate_word]\n","\n","    pos_sim = np.sum(matrix[i].dot(pos_mat.T))\n","    neg_sim = np.sum(matrix[i].dot(neg_mat.T))\n","\n","    return pos_sim - neg_sim\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QtMBF419Fyy8"},"source":["## Set up sentiment model"]},{"cell_type":"code","metadata":{"id":"jxpW1yG1GAl4"},"source":["import bz2\n","\n","# Read input embeddings\n","glove_home = 'drive/My Drive/Colab Notebooks/2020-2021_labs/data/embeddings/glove.6B.50d.txt.bz2'\n","embsfile = bz2.open(glove_home)\n","words, matrix = read(embsfile)\n","\n","# Length normalize embeddings so their dot product effectively computes the cosine similarity\n","matrix = length_normalize(matrix)\n","\n","# Build word to index map\n","word2ind = {word: i for i, word in enumerate(words)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vH3FPdmlF4eQ"},"source":["seed_pos = ['good', 'great', 'awesome', 'like', 'love']\n","seed_neg = ['bad', 'awful', 'terrible', 'hate', 'dislike']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0rwW75RF7A0"},"source":["print(determine_coefficient('abhorrent', seed_pos, seed_neg))\n","print(determine_coefficient('vacations', seed_pos, seed_neg))\n","print(determine_coefficient('hunger', seed_pos, seed_neg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDZkDBGOGUQs"},"source":["## Apply sentiment analysis"]},{"cell_type":"markdown","metadata":{"id":"fp7Q3_05ccRk"},"source":["## Exercise 3\n","- Build sentiment analysis model using determine_coefficient function and aggregate the score of each word in the input sentence (preprocessed or not). \n","\n","- You have to complete the code for `predict_sentiment`, which takes the preprocessed dataframe as input, and return predicted sentiments as well as gold sentiments. "]},{"cell_type":"code","metadata":{"id":"111iroeoWfcI"},"source":["def predict_sentiment(data):\n","    ## YOUR CODE HERE\n","\n","    return sentiments, gold_sentiments\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMDdCSZ0bPCe"},"source":["from sklearn.metrics import accuracy_score\n","\n","pred_sentiments, gold_sentimens = predict_sentiment(preproc_training)\n","accuracy_score(pred_sentimens, gold_sentiments)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOE-0BgqLrqD"},"source":["# 5. EDA: Easy Data Augmentation \n","We will use the EDA package available in Github: https://github.com/jasonwei20/eda_nlp . This code make some transformation on the input sentence obtain a similar, but different extra examples. \n","\n","- **Synonym Replacement (SR)**: Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n","- **Random Insertion (RI)**: Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n","- **Random Swap (RS)**: Randomly choose two words in the sentence and swap their positions. Do this n times.\n","- **Random Deletion (RD)**: For each word in the sentence, randomly remove it with probability p.\n","\n","Note that these transformations are useful for text classification tasks such as sentiment analysis. "]},{"cell_type":"code","metadata":{"id":"vcI21RE3X810"},"source":["!git clone https://github.com/jasonwei20/eda_nlp.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQcUCWO6YLha"},"source":["import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbndVIRUYlUI"},"source":["!python eda_nlp/code/augment.py --input=eda_nlp/data/sst2_train_500.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Qqs5S3_bGmF"},"source":["### Exercise 4\n","- Inspect and analyse the `eda_nlp/data/eda_sst2_train_500.txt` file. Can you identify example for each of transformations?"]},{"cell_type":"markdown","source":["### Exercise 5\n","In this exercise we are going to run some experiments on sentiment analysis to see if EDA works when we only have few annotated examples to run.\n","\n","In order to simulate a scenario with little annotated data, we are going to make the following steps:\n","\n","- Create a small dataset of 100 examples from SST dataset.\n","- Train and evaluate the baseline model \n","- Augment training set with EDA (TODO)\n","- Train and evaluate a new model in the augmented dataset. (TODO)"],"metadata":{"id":"IIzPWWqaxcS8"}},{"cell_type":"markdown","source":["#### Create small dataset of sentiment analysis\n"],"metadata":{"id":"SpI-JJzUzOIs"}},{"cell_type":"code","source":["# Create small dataset of sentiment analysis\n","positive_examples = training_set[training_set.label == 1].sample(50)\n","negative_examples = training_set[training_set.label == 0].sample(50)\n","\n","small_training = pd.concat([positive_examples, negative_examples], axis=0).sample(frac=1)\n","small_training.to_csv(\"small_sentiment_training_set.txt\", sep=\"\\t\",\n","                      columns=[\"label\", \"text\"], header=False, index=False)"],"metadata":{"id":"poMxA4OQzcDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Train and evaluate\n","Which means: preprocess the data, define the model, run the model on the dataset."],"metadata":{"id":"--X5ttcvzjvD"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# prepare dataset (train/dev)\n","vocab = small_training['text'].str.split(expand=True).unstack().value_counts()\n","max_features = len(vocab)\n","sequence_length = 40\n","batch_size = 32\n","\n","vectorize_layer = tf.keras.layers.TextVectorization(\n","    max_tokens=max_features,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)\n","\n","def vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1)\n","  return vectorize_layer(text), label\n","\n","raw_train_ds = tf.data.Dataset.from_tensor_slices((small_training.text, small_training.label))\n","train_text = raw_train_ds.map(lambda x, y: x)\n","vectorize_layer.adapt(train_text)\n","\n","raw_dev_ds = tf.data.Dataset.from_tensor_slices((dev_set.text, dev_set.label))\n","\n","train_ds = raw_train_ds.batch(batch_size).map(vectorize_text)\n","dev_ds = raw_dev_ds.batch(batch_size).map(vectorize_text)\n","\n","\n","dataset = raw_train_ds.batch(batch_size)\n","text_batch, label_batch = next(iter(dataset))\n","first_review, first_label = text_batch[0], label_batch[0]\n","print(\"Review\", first_review)\n","print(\"Label\", first_label)\n","print(\"Vectorized review\", vectorize_text(first_review, first_label))\n","print(\"Vocab size\", max_features)"],"metadata":{"id":"H60filwzzmrz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the model for text classification with keras"],"metadata":{"id":"E-6zyzLN0oOB"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","embedding_dim = 300\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.GlobalAveragePooling1D(),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(1)])\n","\n","model.summary()"],"metadata":{"id":"Jzhri9To0psZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fit the model and evaluate on validation"],"metadata":{"id":"fVaS3_xz0ylY"}},{"cell_type":"code","source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer='adam',\n","              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n","\n","train_ds.batch(batch_size)\n","model.fit(train_ds, epochs=20, validation_data = dev_ds)"],"metadata":{"id":"QGtHzNOV00aV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Apply EDA on the small dataset\n","\n","__TODO__: Run the EDA script in the newly created small dataset (\"small_sentiment_training_set.txt\") and create an augmented dataset. \n","\n","You can run the following command see the option for EDA.\n","```\n","! python eda_nlp/code/augmented.py --help\n","```"],"metadata":{"id":"4Y722zzD040K"}},{"cell_type":"code","source":["# ADD YOUR CODE HERE: Create the augmented dataset\n"],"metadata":{"id":"SZdKaZog08bm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__TODO__: Load the dataset, prepare the training set, define the model and train it (you can repeat the code used above).\n","\n","__TODO__: Check if augmented data improve the results"],"metadata":{"id":"u4Ao0BuT2Cys"}},{"cell_type":"code","source":["# ADD YOUR CODE HERE:\n","import tensorflow as tf\n","\n","# load dataset\n","\n","# prepare new training set (and dev set)\n"],"metadata":{"id":"voq6TqPm2HOG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__TODO__: Define the model and fit it using the augmented dataset (you can repeate the code used for the baseline model):"],"metadata":{"id":"vinvOD6X2UFL"}},{"cell_type":"code","source":["# ADD YOUR CODE HERE:\n","import tensorflow as tf\n","\n","# model definition\n","\n","# fit the model"],"metadata":{"id":"ewkX_heo2Xdi"},"execution_count":null,"outputs":[]}]}