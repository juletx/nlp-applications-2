{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9. IR_for_covid19.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNV1QryDC1v+WuSp9WBiFMb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FoLv7ol1GgPf"},"source":["#I.R. for Covid19\n","\n","Based on https://www.kaggle.com/aotegi/neural-question-answering-for-cord19-task8\n","(deepest apreciation to Jon Ander Campos and Arantxa Otegi, winners of task 'What do we know about diagnostics and surveillance?' in COVID-19 Open Research Dataset Challenge (CORD-19))\n","\n","The goal of this lab is to build a I.R. system that retrieves the most relevant documents given a query related to Covid19.\n","\n","We only use the freely available [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research), which contains metadata of over 51,000 scientific papers (full text is also available for around 40,000 of them) about COVID-19, SARS-CoV-2, and related coronaviruses.\n","\n","As we are mostly interested in papers related to COVID-19 (and not other coronaviruses), we filter out papers that are about coronaviruses other than COVID-19 (for example, SARS-CoV and MERS).\n","\n","The system has a main component that is an Information Retrieval system (IR), based on the classical BM25F search algorithm. This system indexes abstracts and paragraphs on the full text of the papers."]},{"cell_type":"markdown","metadata":{"id":"NbeloDtU12Xm"},"source":["## 1. Install packages and load libraries<a class=\"anchor\" id=\"libraries\"></a>\n","\n","In this section we will install all the packages and load all the libraries needed to run the code below."]},{"cell_type":"code","metadata":{"trusted":true,"id":"RpssaLrz12Xo"},"source":["!pip install Whoosh # search engine library"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"id":"9QdXYQjJ12Xp"},"source":["import codecs # base classes for standard Python codecs, like text encodings (UTF-8,...)\n","from IPython.core.display import display, HTML # object displaying in different formats\n","from whoosh.index import * # whoosh: full-text indexing and searching\n","from whoosh.fields import *\n","from whoosh import qparser\n","import glob\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DG4VlfrN12Xq"},"source":["## 2. Load info from data file<a class=\"anchor\" id=\"files\"></a>\n","\n","CORD19-dataset includes research papers related to coronavirus and COVID-19. In this section we first load the info. As we are not interested in all the metadata info from papers, we will select just text information, such as title, abstract and body text (already done for you).\n","\n","CORD-19.v7 includes info of 51,078 papers, but some of them are repeated (they have the same *cord_uid*). Thus, we already filter out the repeated ones. \n","\n","As we are mostly interested in papers related to COVID-19 (and not other coronaviruses), we want to filter out papers that are about coronaviruses other than COVID-19 (for example, SARS-CoV and MERS). For that purpose, we created a list of synonyms of COVID-19 and we check if a synonym appears in the title or the abstract of a paper. \n","\n","List of synonyms used for filtering:\n","\n","    'coronavirus 2019',\n","    'coronavirus disease 19',\n","    'cov2',\n","    'cov-2',\n","    'covid',\n","    'ncov 2019',\n","    '2019ncov',\n","    '2019-ncov',\n","    '2019 ncov',\n","    'novel coronavirus',\n","    'sarscov2',\n","    'sars-cov-2',\n","    'sars cov 2',\n","    'severe acute respiratory syndrome coronavirus 2',\n","    'wuhan coronavirus',\n","    'wuhan pneumonia',\n","    'wuhan virus'\n","\n","In that way, we filter out those papers that do not include any of the synonyms. From now on, we will consider only the papers that we keep after filtering.\n","\n","This are the number of papers after filtering:"]},{"cell_type":"code","metadata":{"id":"MK97YhOv3B_v"},"source":["from google.colab import drive "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHTUlkSPEcLH"},"source":["path='drive/My Drive/' #set path to nlp-app-II/data/ -> passages\n","count=0\n","passages=[]\n","with open(path+'passages') as f:\n","    for line in f:\n","      count+=1\n","      passages.append(line)\n","      if count == 5000:\n","          break\n","print(\"Number of passages related to 'COVID-19':\", count)\n","print()\n","print(\"3 Random passages:\")\n","print()\n","print(passages[random.randrange(count)])\n","print(passages[random.randrange(count)])\n","print(passages[random.randrange(count)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJsKd12X12Xt"},"source":["## 3. Create an index for the paper retrieval system <a class=\"anchor\" id=\"index\"></a>\n","\n","The system that we are going to develop in our approach is the information retrieval system. An information retrieval system is a tool that searches for  documents that are relevant to an information need from a collection of documents. This system has two main modules: the indexing system and the query system. \n","\n","The first module is in charge of creating the primary data structure for the system, which is the index. The second component is the one with which users interact submitting a query based on their information need, and based on this query and using the index, retrieves documents. In this section we will create an index, and in the next section, we will develop the query system. For the implementation of these modules, we will use [Whoosh library](https://pypi.org/project/Whoosh/), which contains functions for indexing text and then searching the index.\n","\n","The index is a data structure that makes it possible to search for information in a document collection in a very efficient way. In short, it lists, for every word, all documents that contain it.\n","\n","In order to create an index, we must define the schema of the index. The schema lists the fields in the index. A field is a piece of information for each document in the index, for example, id, path of the document, title and text. We define the type of these last two fields as “TEXT”, which means that they will be searchable. As it is common practice, we also define to apply the Stemming Analyzer to these text fields. Applying this analyzer all the text will be tokenized, then all the tokens will be converted to lowercase, a stopword filter will be applied in order to remove too common words, and finally, a stemming algorithm will be applied."]},{"cell_type":"code","metadata":{"trusted":true,"id":"CzhvaURl12Xt"},"source":["# Schema definition:\n","# - id: type ID, unique, stored; doc id in order given the passages file\n","# - text: type TEXT processed by StemmingAnalyzer; not stored; content of the passage\n","schema = Schema(id = ID(stored=True,unique=True),\n","                text = TEXT(analyzer=analysis.StemmingAnalyzer())\n","               )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8D4lqmmn12Xu"},"source":["Once we have the schema, we can create an index.\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"MJaCeQse12Xu"},"source":["# Create an index\n","if not os.path.exists(\"index\"):\n","    os.mkdir(\"index\")\n","\n","ix = create_in(\"index\", schema)\n","writer = ix.writer() #run once! or restart runtime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjNobShx12Xv"},"source":["Next, we will add documents to the index. We will index the papers related to COVID-19, not only the abstracts that are in the metadata file, but also the full text provided in PMC or PDF JSON format. As having shorter documents is better for the answering system that we will develop later, we will not index the whole text in a paper together. Instead, the indexing unit will be an abstract or each of the paragraphs of the full text (as marked in JSON files).\n","\n","This could take several minutes."]},{"cell_type":"code","metadata":{"trusted":true,"id":"miAgCu6n12Xv"},"source":["# Add papers to the index, iterating through each row in the metadata dataframe\n","\n","\n","for ind,passage_text in enumerate(passages): \n","    writer.add_document(id=str(ind),text=passage_text)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ToAJiX2S12Xx"},"source":["Finally, we will save the added documents to the index."]},{"cell_type":"code","metadata":{"trusted":true,"id":"W6h-iOv-12Xx"},"source":["# Save the added documents\n","writer.commit()\n","print(\"Index successfully created\")\n","\n","# Sanity check\n","print(\"Number of documents (abstracts and paragraphs of papers) in the index: \", ix.doc_count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSvit5Pi12Xx"},"source":["## 4. Define a function to query the index and retrieve relevant papers <a class=\"anchor\" id=\"retrieval\"></a>\n","\n","In this section we will define a function that given a question and a maximum number of documents as input, it uses this query to retrieve relevant papers that were indexed in the previous section.\n","\n","In this function we set the algorithm used for scoring (we will be using the default BM25 algorithm), and we  also set the query parser to use, defining the default field to search (in our case '*text*’ field). Then, we run the query and get the most relevant documents on the index (*n_docs* documents at maximum). \n","\n","The output of the function is a set (*n_docs*) of texts and scores."]},{"cell_type":"code","metadata":{"trusted":true,"id":"e6T5TJkQ12Xy"},"source":["# Input: Question and maximum number of documents to retrieve\n","def retrieve_docs(qstring, n_docs):\n","    scores=[]\n","    text=[]\n","    # Open the searcher for reading the index. The default BM25 algorithm will be used for scoring\n","    with ix.searcher() as searcher:\n","        searcher = ix.searcher()\n","        \n","        # Define the query parser ('text' will be the default field to search), and set the input query\n","        q = qparser.QueryParser(\"text\", ix.schema, group=qparser.OrGroup).parse(qstring)\n","    \n","        # Search using the query q, and get the n_docs documents, sorted with the highest-scoring documents first\n","        results = searcher.search(q, limit=n_docs)\n","        # results is a list of dictionaries where each dictionary is the stored fields of the document\n","  \n","    # Iterate over the retrieved documents\n","    for hit in results:\n","        scores.append(hit.score)\n","        text.append(passages[int(hit['id'])])\n","    return text,scores\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Rk9E3b18zXv"},"source":["Retrieve 3 most relevant documents and scores given the query \"How long individuals are contagious?\":"]},{"cell_type":"code","metadata":{"id":"j25PpOsoQq4C"},"source":["retrieve_docs(\"How long individuals are contagious?\",3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n18hCmN59GSm"},"source":["Test as many queries you want:\n","\n","- Range of incubation periods for the disease in humans\n","- Prevalence of asymptomatic shedding and transmission\n","- Persistence of virus on surfaces of different materials\n","- Immune response and immunity\n","- Does smoking increase risk for COVID-19?\n","- Risk of fatality among symptomatic hospitalized patients\n","- Efforts targeted at a universal coronavirus vaccine\n","- What is known about the efficacy of school closures?\n","-Is there any evidence to suggest geographic based virus mutations?\n","\n","Check more queries in section 6 https://www.kaggle.com/aotegi/neural-question-answering-for-cord19-task8\n","\n","Are the retrieved documents relevant for each query?"]},{"cell_type":"markdown","metadata":{"id":"73y56nAf-HjO"},"source":["If you want to test this model in other domain, load your own *passages* file. In *passages* we store one passage per line. You can try to create your own *passages* file, for example, using Wikipedia abstracts (1st paragraph of each Wiki page) or SQuAD dataset passages. Feel free to use this data or your own *passages* in the assignment."]}]}